{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to scrape the location of supermarkets in the UK. This will be used in the future to build a dashboard of the food supply chain in the UK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Scraping the links of all the cities by letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub repository:\n",
    "    \n",
    "https://github.com/tlemenestrel/Mining_Food_Supply_Chain_Data\n",
    "\n",
    "Link to the scraped website:\n",
    "\n",
    "https://openhours.co.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links to the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "website_letters_link = \"https://openhours.co.uk/categories/supermarket-583/choose_location?all_locations=true&on=\"\n",
    "website_cities_link  = \"https://openhours.co.uk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of all the letters for the pages to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_letters = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"Y\",\"Z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List to append afterwards with the name of the cities and their respective links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities = []\n",
    "cities_links = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the cities and their links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for letter in list_letters:\n",
    "    \n",
    "    # Requesting the page using the url plus the associated letter\n",
    "    \n",
    "    page = requests.get(website_letters_link + letter)\n",
    "    \n",
    "    # Turning the page into a soup for future scraping\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "        \n",
    "    # Finding the name of the city and its link\n",
    "    \n",
    "    for j in soup.find_all('li'):\n",
    "                \n",
    "        a = j.find ('a')\n",
    "        \n",
    "        # Appending the list of cities and their respective links\n",
    "\n",
    "        cities_links.append(website_cities_link + a.attrs['href'])\n",
    "        cities.append(a.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the lists of scraped data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities_df = pd.DataFrame({\n",
    "    \n",
    "    \"cities_links\": cities_links,\n",
    "    \"cities\"      : cities\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the entire dataframe to check if the scraped data is accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cities_links               cities\n",
      "0                 https://openhours.co.uk/categories       [[Categories]]\n",
      "1  https://openhours.co.uk/categories/supermarket...      [[Supermarket]]\n",
      "2  https://openhours.co.uk/categories/supermarket...  [[Choose location]]\n",
      "3  https://openhours.co.uk/categories/supermarket...                  [A]\n",
      "4  https://openhours.co.uk/categories/supermarket...                  [B]\n"
     ]
    }
   ],
   "source": [
    "print (cities_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the dataframe to only keep the links of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities_df = cities_df[cities_df['cities_links'].str.contains('/spots')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that the dataframe only contains the links of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cities_links            cities\n",
      "28  https://openhours.co.uk/spots?city=Abbey+Road&...      [Abbey Road]\n",
      "29  https://openhours.co.uk/spots?city=Abbey+Wood&...      [Abbey Wood]\n",
      "30  https://openhours.co.uk/spots?city=Abbots+Lang...  [Abbots Langley]\n",
      "31  https://openhours.co.uk/spots?city=Aberdare&la...        [Aberdare]\n",
      "32  https://openhours.co.uk/spots?city=Aberdeen&la...        [Aberdeen]\n"
     ]
    }
   ],
   "source": [
    "print (cities_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the type of each column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_data_type_of_all_columns_of_a_dataframe(df):\n",
    "\n",
    "    dataTypeSeries = df.dtypes\n",
    "    print('Data type of each column of the dataframe :')\n",
    "    print(dataTypeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of each column of the dataframe :\n",
      "cities_links    object\n",
      "cities          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print_data_type_of_all_columns_of_a_dataframe(cities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the brackets in the cities column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities_df['cities'] = cities_df['cities'].str.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that the column has been properly changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cities_links          cities\n",
      "28  https://openhours.co.uk/spots?city=Abbey+Road&...      Abbey Road\n",
      "29  https://openhours.co.uk/spots?city=Abbey+Wood&...      Abbey Wood\n",
      "30  https://openhours.co.uk/spots?city=Abbots+Lang...  Abbots Langley\n",
      "31  https://openhours.co.uk/spots?city=Aberdare&la...        Aberdare\n",
      "32  https://openhours.co.uk/spots?city=Aberdeen&la...        Aberdeen\n"
     ]
    }
   ],
   "source": [
    "print (cities_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities_df.to_csv('cities.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Scraping the data from all the links of the different cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping an individual page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the task ahead, we will start by scraping an individual page and later integrate it into a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "individual_supermarket_name = []\n",
    "individual_supermarket_address = []\n",
    "individual_supermarket_postcode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YO24 2RQ', 'DN8 5BT', 'DN8 5BA', 'DN11 9HT', 'DN11 9HT', 'YO23 2RA', 'YO24 3JQ', 'SK14 2TA', 'SK14 1HB', 'OL9 8AU']\n"
     ]
    }
   ],
   "source": [
    "page = requests.get(\"https://openhours.co.uk/spots?city=Ossett&lat=53.6798&lng=-1.5801&page=100&q=&search_term_id=583\")\n",
    "\n",
    "# Turning the page into a soup for future scraping\n",
    "\n",
    "postcodes_parsed = SoupStrainer(itemprop = 'postalCode')\n",
    "                                  \n",
    "soup = BeautifulSoup(page.content, \"lxml\", parse_only=postcodes_parsed)\n",
    "\n",
    "# Finding the postal codes of an individual page\n",
    "\n",
    "for postcode in soup.find_all(itemprop = 'postalCode'):\n",
    "    \n",
    "        # Appending the list of cities and their respective links\n",
    "\n",
    "        individual_supermarket_postcode.append(postcode.next)\n",
    "\n",
    "print (individual_supermarket_postcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "individual_postcodes_df = pd.DataFrame({\n",
    "    \n",
    "    \"postcodes\": individual_supermarket_postcode\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  postcodes\n",
      "0  YO24 2RQ\n",
      "1   DN8 5BT\n",
      "2   DN8 5BA\n",
      "3  DN11 9HT\n",
      "4  DN11 9HT\n",
      "5  YO23 2RA\n",
      "6  YO24 3JQ\n",
      "7  SK14 2TA\n",
      "8  SK14 1HB\n",
      "9   OL9 8AU\n"
     ]
    }
   ],
   "source": [
    "print (individual_postcodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the new list of URLs from the data scraped previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to scrape one page, we can start to scrape all the other pages and generalize our approach to the entire website. For this, we will need to generate a list with all the new URLs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging the previous URLs by removing extra characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String to remove at the end of each URLs:\n",
    "    \n",
    "q=&search_term_id=583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities_df['cities_links'] = cities_df['cities_links'].str.rstrip('q=&search_term_id=583')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that the characters have been removed at the end of each URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 cities_links  \\\n",
      "28      https://openhours.co.uk/spots?city=Abbey+Road&lat=51.5299&lng=-0.1747   \n",
      "29        https://openhours.co.uk/spots?city=Abbey+Wood&lat=51.4869&lng=0.107   \n",
      "30  https://openhours.co.uk/spots?city=Abbots+Langley&lat=51.7057&lng=-0.4176   \n",
      "31        https://openhours.co.uk/spots?city=Aberdare&lat=51.7144&lng=-3.4492   \n",
      "32        https://openhours.co.uk/spots?city=Aberdeen&lat=57.1437&lng=-2.0981   \n",
      "\n",
      "            cities  \n",
      "28      Abbey Road  \n",
      "29      Abbey Wood  \n",
      "30  Abbots Langley  \n",
      "31        Aberdare  \n",
      "32        Aberdeen  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(cities_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the new list of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the previous dataframe into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_links = cities_df['cities_links'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking that the list is of the right size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1390\n"
     ]
    }
   ],
   "source": [
    "print (len(list_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the new list of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in list_links :\n",
    "        \n",
    "    for i in range (1,101) :\n",
    "        \n",
    "            # Requesting the page using the URL plus the associated letter\n",
    "    \n",
    "            URLs.append(link + '&page=' + str(i) + '&q=&search_term_id=583')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the size of the new list of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139000\n"
     ]
    }
   ],
   "source": [
    "print(len(URLs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing an example of an URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openhours.co.uk/spots?city=Abbey+Road&lat=51.5299&lng=-0.1747&page=1&q=&search_term_id=583\n"
     ]
    }
   ],
   "source": [
    "print(URLs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a complete list of URLs, we can start to implement multi-threading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a list of goods reads on multi-threading and the number of threads to use and also the difference between multi-threading and multi-processing.\n",
    "\n",
    "<br/>\n",
    "\n",
    "How to implement multi-threading and multi-processing for a web scraper:\n",
    "\n",
    "https://medium.com/@apbetahouse45/asynchronous-web-scraping-in-python-using-concurrent-module-a5ca1b7f82e4\n",
    "\n",
    "Right number of threads to use:\n",
    "\n",
    "https://www.jstorimer.com/blogs/workingwithcode/7970125-how-many-threads-is-too-many\n",
    "\n",
    "Difference between multi-processing and multi-threading:\n",
    "\n",
    "https://medium.com/contentsquare-engineering-blog/multithreading-vs-multiprocessing-in-python-ece023ad55a\n",
    "\n",
    "<br/>\n",
    "\n",
    "In this case, I used multi-threading, as this task was I/O and not CPU bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the difference in performance using multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use a subset of the list of URLs and determine what is the best number of workers for multi-threading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Changed to verify = False to disable SSL as I used to get an error because of it\n",
    "\n",
    "-Used SoupStrainer to only scrape specific tags to improve the performance of the scraper\n",
    "\n",
    "-Added a time sleep of a total of 10ms for 10,000 URLs to slow down the scraping and the pressure put on the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a subset of 1,000 URLs to be used for performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs_subset = URLs[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_postcode(url):\n",
    "    \n",
    "    # Adding a pause of 10 milliseconds every time the function is called - adding 10 seconds of delay in total \n",
    "    \n",
    "    time.sleep(1 * 10**(-2))\n",
    "    \n",
    "    supermarkets_postcode = []\n",
    "    \n",
    "    # Requesting the page using the URL plus the associated letter\n",
    "\n",
    "    page = requests.get(url, verify = False)\n",
    "\n",
    "    # Turning the page into a soup for future scraping\n",
    "\n",
    "    postcodes_parsed = SoupStrainer(itemprop = 'postalCode')\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"lxml\", parse_only=postcodes_parsed)\n",
    "\n",
    "    # Finding the name of the city and its link\n",
    "    \n",
    "    for postcode in soup.find_all(itemprop = 'postalCode'):\n",
    "    \n",
    "        # Appending the list of cities and their respective links\n",
    "\n",
    "        supermarkets_postcode.append(postcode.next)\n",
    "        \n",
    "    return supermarkets_postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pool with 10 workers for multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "pool = Pool(10)\n",
    "results = pool.map(scrape_postcode, URLs_subset)\n",
    "        \n",
    "pool.close()\n",
    "pool.join()\n",
    "        \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the time it took to scrape all the links with 10 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 62.081752s\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken: {:.6f}s\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pool with 15 workers for multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "pool = Pool(15)\n",
    "results = pool.map(scrape_postcode, URLs_subset)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "        \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the time it took to scrape all the links with 15 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 70.302282s\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken: {:.6f}s\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pool with 20 workers for multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "pool = Pool(20)\n",
    "results = pool.map(scrape_postcode, URLs_subset)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "        \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the time it took to scrape all the links with 20 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 79.616263s\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken: {:.6f}s\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pool with 25 workers for multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "pool = Pool(25)\n",
    "results = pool.map(scrape_postcode, URLs_subset)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "        \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the time it took to scrape all the links with 25 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 84.469282s\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken: {:.6f}s\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pool with 30 workers for multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "pool = Pool(30)\n",
    "results = pool.map(scrape_postcode, URLs_subset)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "        \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the time it took to scrape all the links with 30 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 78.566728s\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken: {:.6f}s\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pool with 100 workers for multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "pool = Pool(100)\n",
    "results = pool.map(scrape_postcode, URLs_subset)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "        \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the time it took to scrape all the links with 100 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 79.337509s\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken: {:.6f}s\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the scraping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NW8 6PG', 'W9 1UP', 'NW8 8JN', 'W9 1SY', 'NW8 8EX', 'NW1 6TU', 'W2 1DY', 'W2 6EZ', 'W2 1EE', 'NW1 6AE'], ['NW6 4RY', 'NW6 5UA', 'W2 1HB', 'W2 2DS', 'NW3 3NT', 'NW3 3RA', 'W2 2DS', 'W1U 6TS', 'W1U 6TP', 'W2 1RH'], ['NW3 6JR', 'NW6 4HS', 'W2 3PX', 'W2 6ES', 'NW6 6JH', 'W2 2EA', 'NW3 6NN', 'W2 3QA', 'W2 4SB', 'W1U 8NN'], ['NW1 4SA', 'W9 3RU', 'W1U 4SD', 'NW3 4UE', 'W1U 3AA', 'W9 3QA', 'W1U 4SA', 'W2 5RT', 'W1H 7AA', 'NW3 6LU'], ['NW6 6NL', 'W2 3RL', 'W2 4QH', 'W2 3RY', 'NW6 7JN', 'NW1 7AH', 'W2 3HJ', 'NW1 8AN', 'NW1 3AU', 'NW1 7PN'], ['NW6 7TA', 'NW6 6RG', 'W10 4RG', 'W1W 5QQ', 'NW3 4QG', 'NW1 8AA', 'NW1 7JR', 'N1 9LE', 'NW6 1SG', 'NW1 0LU'], ['W1C 2JS', 'NW1 0LT', 'W1W 6PS', 'NW1 0LT', 'NW6 1SG', 'W1W 6BE', 'NW1 3JA', 'NW1 9LJ', 'W11 1LJ', 'W11 1LA'], ['NW6 7QE', 'NW1 0JH', 'W1A 1EX', <span class=\"postal-code\" itemprop=\"postalCode\">NW5 4EG</span>, 'NW5 4EG', 'W1T 5AS', 'NW6 7QE', 'W1K 2PX', 'NW6 1RN', 'W1T 7NE'], ['NW3 2YY', 'W10 6HH', 'W10 6HJ', 'W11 3QG', 'W11 3QE', 'NW1 1TT', 'NW1 2DU', 'NW3 6TR', 'W1T 3JG', 'W10 5AA'], ['NW5 2JU', 'W1T 2QB', 'NW6 1NY', 'NW10 3NB', 'NW5 2JU', 'N18 2JB', 'NW1 1HY', 'W1B 5BT', 'W1T 7PT', 'W1T 3LA']]\n",
      "139000\n"
     ]
    }
   ],
   "source": [
    "print (results[:10])\n",
    "print (len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the scraper does not increase after using more than 10 workers. Therefore, we will keep this amount of workers for scraping all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping all the data using multi-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the function to scrape the data of an individual page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_postcode(url):\n",
    "    \n",
    "    supermarkets_postcode = []\n",
    "    \n",
    "    # Requesting the page using the URL plus the associated letter\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    # Turning the page into a soup for future scraping\n",
    "\n",
    "    postcodes_parsed = SoupStrainer(itemprop = 'postalCode')\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"lxml\", parse_only=postcodes_parsed)\n",
    "\n",
    "    # Finding the name of the city and its link\n",
    "    \n",
    "    for postcode in soup.find_all(itemprop = 'postalCode'):\n",
    "    \n",
    "        # Appending the list of cities and their respective links\n",
    "\n",
    "        supermarkets_postcode.append(postcode.next)\n",
    "        \n",
    "    return supermarkets_postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the pool for multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "pool = Pool(30)\n",
    "results = pool.map(scrape_postcode, URLs)\n",
    "\n",
    "pool.terminate()\n",
    "pool.join()\n",
    "        \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the time it took to scrape all the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken: 5506.458607s\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken: {:.6f}s\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the scraping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139000\n",
      "[['NW8 6PG', 'W9 1UP', 'NW8 8JN', 'W9 1SY', 'NW8 8EX', 'NW1 6TU', 'W2 1DY', 'W2 6EZ', 'W2 1EE', 'NW1 6AE'], ['NW6 4RY', 'NW6 5UA', 'W2 1HB', 'W2 2DS', 'NW3 3NT', 'NW3 3RA', 'W2 2DS', 'W1U 6TS', 'W1U 6TP', 'W2 1RH'], ['NW3 6JR', 'NW6 4HS', 'W2 3PX', 'W2 6ES', 'NW6 6JH', 'W2 2EA', 'NW3 6NN', 'W2 3QA', 'W2 4SB', 'W1U 8NN'], ['NW1 4SA', 'W9 3RU', 'W1U 4SD', 'NW3 4UE', 'W1U 3AA', 'W9 3QA', 'W1U 4SA', 'W2 5RT', 'W1H 7AA', 'NW3 6LU'], ['NW6 6NL', 'W2 3RL', 'W2 4QH', 'W2 3RY', 'NW6 7JN', 'NW1 7AH', 'W2 3HJ', 'NW1 8AN', 'NW1 3AU', 'NW1 7PN'], ['NW6 7TA', 'NW6 6RG', 'W10 4RG', 'W1W 5QQ', 'NW3 4QG', 'NW1 8AA', 'NW1 7JR', 'N1 9LE', 'NW6 1SG', 'NW1 0LU'], ['W1C 2JS', 'NW1 0LT', 'W1W 6PS', 'NW1 0LT', 'NW6 1SG', 'W1W 6BE', 'NW1 3JA', 'NW1 9LJ', 'W11 1LJ', 'W11 1LA'], ['NW6 7QE', 'NW1 0JH', 'W1A 1EX', <span class=\"postal-code\" itemprop=\"postalCode\">NW5 4EG</span>, 'NW5 4EG', 'W1T 5AS', 'NW6 7QE', 'W1K 2PX', 'NW6 1RN', 'W1T 7NE'], ['NW3 2YY', 'W10 6HH', 'W10 6HJ', 'W11 3QG', 'W11 3QE', 'NW1 1TT', 'NW1 2DU', 'NW3 6TR', 'W1T 3JG', 'W10 5AA'], ['NW5 2JU', 'W1T 2QB', 'NW6 1NY', 'NW10 3NB', 'NW5 2JU', 'N18 2JB', 'NW1 1HY', 'W1B 5BT', 'W1T 7PT', 'W1T 3LA']]\n"
     ]
    }
   ],
   "source": [
    "print (len(results))\n",
    "print (results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the 2-D list into a 1-D list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes = list(chain.from_iterable(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the first 10 elements of the list to check it has been properly flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NW8 6PG', 'W9 1UP', 'NW8 8JN', 'W9 1SY', 'NW8 8EX', 'NW1 6TU', 'W2 1DY', 'W2 6EZ', 'W2 1EE', 'NW1 6AE']\n"
     ]
    }
   ],
   "source": [
    "print (postcodes[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the list of scraped data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "postcodes_df = pd.DataFrame({\n",
    "    \n",
    "    \"postcodes\": postcodes\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the entire dataframe to check if the scraped data is accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  postcodes\n",
      "0   NW8 6PG\n",
      "1    W9 1UP\n",
      "2   NW8 8JN\n",
      "3    W9 1SY\n",
      "4   NW8 8EX\n",
      "\n",
      "The dataframe contains: 1372000 elements\n"
     ]
    }
   ],
   "source": [
    "print (postcodes_df.head())\n",
    "print (\"\\nThe dataframe contains: \" + str(len(postcodes_df)) + \" elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the types of each columns of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of each column of the dataframe :\n",
      "postcodes    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print_data_type_of_all_columns_of_a_dataframe(postcodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the first 5 rows of the dataframe to check that the white spaces have been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  postcodes\n",
      "0   NW8 6PG\n",
      "1    W9 1UP\n",
      "2   NW8 8JN\n",
      "3    W9 1SY\n",
      "4   NW8 8EX\n"
     ]
    }
   ],
   "source": [
    "print (postcodes_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to print the number of rows the dataframe contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_rows_a_dataframe_contains(df):\n",
    "    \n",
    "    print (\"\\nThe dataframe contains: \" + str(len(df)) + \" elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all the duplicates from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataframe contains: 17415 elements\n"
     ]
    }
   ],
   "source": [
    "postcodes_df = postcodes_df.drop_duplicates(keep='first')\n",
    "print_number_of_rows_a_dataframe_contains(postcodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the rows containing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataframe contains: 17414 elements\n"
     ]
    }
   ],
   "source": [
    "postcodes_df = postcodes_df.dropna()\n",
    "print_number_of_rows_a_dataframe_contains(postcodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the rows that start with <span to drop all the HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataframe contains: 15904 elements\n"
     ]
    }
   ],
   "source": [
    "postcodes_df = postcodes_df[~postcodes_df['postcodes'].astype(str).str.startswith('<span')]\n",
    "print_number_of_rows_a_dataframe_contains(postcodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows that do not only have uppercase letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataframe contains: 15803 elements\n"
     ]
    }
   ],
   "source": [
    "postcodes_df = postcodes_df[postcodes_df['postcodes'].str.isupper()]\n",
    "print_number_of_rows_a_dataframe_contains(postcodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows that are between 6 and 8 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As postcodes in the UK are between 6 and 8 characters, this will remove a lot of bad rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataframe contains: 15728 elements\n"
     ]
    }
   ],
   "source": [
    "postcodes_df = postcodes_df[postcodes_df['postcodes'].str.len() <= 8]\n",
    "postcodes_df = postcodes_df[postcodes_df['postcodes'].str.len() >= 6]\n",
    "print_number_of_rows_a_dataframe_contains(postcodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows that do not have a whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataframe contains: 15686 elements\n"
     ]
    }
   ],
   "source": [
    "postcodes_df= postcodes_df[postcodes_df['postcodes'].str.contains(' ')]\n",
    "#postcodes_df = postcodes_df[postcodes_df['postcodes'].str.isalpha()]\n",
    "print_number_of_rows_a_dataframe_contains(postcodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting the values of the dataframe alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_df = postcodes_df.sort_values(by=['postcodes'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "postcodes_df.to_csv('supermarkets_postcodes.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
