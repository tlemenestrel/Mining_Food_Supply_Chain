{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to scrape the location of supermarkets in the UK. This will be used in the future to build a dashboard of the food supply chain in the UK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Scraping the links of all the cities by letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub repository:\n",
    "    \n",
    "https://github.com/tlemenestrel/Mining_Food_Supply_Chain_Data\n",
    "\n",
    "Link to the scraped website:\n",
    "\n",
    "https://openhours.co.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links to the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_letters_link = \"https://openhours.co.uk/categories/supermarket-583/choose_location?all_locations=true&on=\"\n",
    "website_cities_link  = \"https://openhours.co.uk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of all the letters for the pages to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_letters = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"Y\",\"Z\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List to append afterwards with the name of the cities and their respective links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = []\n",
    "cities_links = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the cities and their links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in list_letters:\n",
    "    \n",
    "    # Requesting the page using the url plus the associated letter\n",
    "    \n",
    "    page = requests.get(website_letters_link + letter)\n",
    "    \n",
    "    # Turning the page into a soup for future scraping\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, 'html5lib')\n",
    "        \n",
    "    # Finding the name of the city and its link\n",
    "    \n",
    "    for j in soup.find_all('li'):\n",
    "                \n",
    "        a = j.find ('a')\n",
    "        \n",
    "        # Appending the list of cities and their respective links\n",
    "\n",
    "        cities_links.append(website_cities_link + a.attrs['href'])\n",
    "        cities.append(a.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the lists of scraped data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = pd.DataFrame({\n",
    "    \n",
    "    \"cities_links\": cities_links,\n",
    "    \"cities\"      : cities\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the entire dataframe to check if the scraped data is accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                 cities_links  \\\n",
      "0                                                          https://openhours.co.uk/categories   \n",
      "1                       https://openhours.co.uk/categories/supermarket-583/choose_subcategory   \n",
      "2  https://openhours.co.uk/categories/supermarket-583/choose_location?all_locations=true&on=A   \n",
      "3  https://openhours.co.uk/categories/supermarket-583/choose_location?all_locations=true&on=A   \n",
      "4  https://openhours.co.uk/categories/supermarket-583/choose_location?all_locations=true&on=B   \n",
      "\n",
      "                cities  \n",
      "0       [[Categories]]  \n",
      "1      [[Supermarket]]  \n",
      "2  [[Choose location]]  \n",
      "3                  [A]  \n",
      "4                  [B]  \n"
     ]
    }
   ],
   "source": [
    "print (cities_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the dataframe to only keep the links of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = cities_df[cities_df['cities_links'].str.contains('/spots')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that the dataframe only contains the links of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                    cities_links  \\\n",
      "28      https://openhours.co.uk/spots?city=Abbey+Road&lat=51.5299&lng=-0.1747&search_term_id=583   \n",
      "29       https://openhours.co.uk/spots?city=Abbey+Wood&lat=51.4869&lng=0.1075&search_term_id=583   \n",
      "30  https://openhours.co.uk/spots?city=Abbots+Langley&lat=51.7057&lng=-0.4176&search_term_id=583   \n",
      "31        https://openhours.co.uk/spots?city=Aberdare&lat=51.7144&lng=-3.4492&search_term_id=583   \n",
      "32        https://openhours.co.uk/spots?city=Aberdeen&lat=57.1437&lng=-2.0981&search_term_id=583   \n",
      "\n",
      "              cities  \n",
      "28      [Abbey Road]  \n",
      "29      [Abbey Wood]  \n",
      "30  [Abbots Langley]  \n",
      "31        [Aberdare]  \n",
      "32        [Aberdeen]  \n"
     ]
    }
   ],
   "source": [
    "print (cities_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the type of each column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_type_of_all_columns_of_a_dataframe(df):\n",
    "\n",
    "    dataTypeSeries = df.dtypes\n",
    "    print('Data type of each column of the dataframe :')\n",
    "    print(dataTypeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of each column of the dataframe :\n",
      "cities_links    object\n",
      "cities          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print_data_type_of_all_columns_of_a_dataframe(cities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the brackets in the cities column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df['cities'] = cities_df['cities'].str.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that the column has been properly changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                    cities_links  \\\n",
      "28      https://openhours.co.uk/spots?city=Abbey+Road&lat=51.5299&lng=-0.1747&search_term_id=583   \n",
      "29       https://openhours.co.uk/spots?city=Abbey+Wood&lat=51.4869&lng=0.1075&search_term_id=583   \n",
      "30  https://openhours.co.uk/spots?city=Abbots+Langley&lat=51.7057&lng=-0.4176&search_term_id=583   \n",
      "31        https://openhours.co.uk/spots?city=Aberdare&lat=51.7144&lng=-3.4492&search_term_id=583   \n",
      "32        https://openhours.co.uk/spots?city=Aberdeen&lat=57.1437&lng=-2.0981&search_term_id=583   \n",
      "\n",
      "            cities  \n",
      "28      Abbey Road  \n",
      "29      Abbey Wood  \n",
      "30  Abbots Langley  \n",
      "31        Aberdare  \n",
      "32        Aberdeen  \n"
     ]
    }
   ],
   "source": [
    "print (cities_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df.to_csv('cities.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Scraping the data from all the links of the different cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping an individual page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the task ahead, we will start by scraping an individual page and later on integrate it into a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_supermarket_name = []\n",
    "individual_supermarket_address = []\n",
    "individual_supermarket_postcode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YO24 2RQ', 'DN8 5BT', 'DN8 5BA', 'DN11 9HT', 'DN11 9HT', 'YO23 2RA', 'YO24 3JQ', 'SK14 2TA', 'SK14 1HB', 'OL9 8AU']\n"
     ]
    }
   ],
   "source": [
    "page = requests.get(\"https://openhours.co.uk/spots?city=Ossett&lat=53.6798&lng=-1.5801&page=100&q=&search_term_id=583\")\n",
    "\n",
    "# Turning the page into a soup for future scraping\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html5lib')\n",
    "\n",
    "# Finding the postal codes of an individual page\n",
    "\n",
    "for postcode in soup.find_all(itemprop = 'postalCode'):\n",
    "    \n",
    "        # Appending the list of cities and their respective links\n",
    "\n",
    "        individual_supermarket_postcode.append(postcode.next)\n",
    "\n",
    "print(individual_supermarket_postcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_postcodes_df = pd.DataFrame({\n",
    "    \n",
    "    \"postcodes\": individual_supermarket_postcode\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  postcodes\n",
      "0  YO24 2RQ\n",
      "1   DN8 5BT\n",
      "2   DN8 5BA\n",
      "3  DN11 9HT\n",
      "4  DN11 9HT\n",
      "5  YO23 2RA\n",
      "6  YO24 3JQ\n",
      "7  SK14 2TA\n",
      "8  SK14 1HB\n",
      "9   OL9 8AU\n"
     ]
    }
   ],
   "source": [
    "print (individual_postcodes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping all the pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to scrape one page, we can start to scrape all the other pages and generalize our approach to the entire website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time and randint are used to delay the scraping, which is done to avoid crashing the website's server\n",
    "\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List to store the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarkets_postcode = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging the previous URLs by removing extra characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String to remove at the end of each URLs:\n",
    "    \n",
    "q=&search_term_id=583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df['cities_links'] = cities_df['cities_links'].str.rstrip('q=&search_term_id=583')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying that the characters have been removed at the end of each URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 cities_links  \\\n",
      "28      https://openhours.co.uk/spots?city=Abbey+Road&lat=51.5299&lng=-0.1747   \n",
      "29        https://openhours.co.uk/spots?city=Abbey+Wood&lat=51.4869&lng=0.107   \n",
      "30  https://openhours.co.uk/spots?city=Abbots+Langley&lat=51.7057&lng=-0.4176   \n",
      "31        https://openhours.co.uk/spots?city=Aberdare&lat=51.7144&lng=-3.4492   \n",
      "32        https://openhours.co.uk/spots?city=Aberdeen&lat=57.1437&lng=-2.0981   \n",
      "\n",
      "            cities  \n",
      "28      Abbey Road  \n",
      "29      Abbey Wood  \n",
      "30  Abbots Langley  \n",
      "31        Aberdare  \n",
      "32        Aberdeen  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(cities_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in cities_links :\n",
    "    \n",
    "    sleep(randint(2,10))\n",
    "    \n",
    "    for i in range (1,100) :\n",
    "        \n",
    "            # Requesting the page using the URL plus the associated letter\n",
    "    \n",
    "            page = requests.get(link + '&page=' + str(i) + '&q=&search_term_id=583')\n",
    "    \n",
    "            # Turning the page into a soup for future scraping\n",
    "    \n",
    "            soup = BeautifulSoup(page.content, 'html5lib')\n",
    "        \n",
    "            # Finding the name of the city and its link\n",
    "    \n",
    "            for postcode in soup.find_all(itemprop = 'postalCode'):\n",
    "    \n",
    "                # Appending the list of cities and their respective links\n",
    "\n",
    "                supermarkets_postcode.append(postcode.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the list of scraped data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_df = pd.DataFrame({\n",
    "    \n",
    "    \"postcodes\": supermarkets_postcode\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the entire dataframe to check if the scraped data is accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (postcodes_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_df.to_csv('supermarkets_postcodes.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
